import sys #standard libraries
import os
import json

from pydantic import BaseModel #3rd party libraries to define data structure
from llm_sdk import Small_LLM_Model
import numpy as np

def build_tool_dict(schema):
    tool_dict = {}
    for unit in schema:
       tool_dict[unit["fn_name"]] = {
        "args_names": unit["args_names"],
        "args_types": unit["args_types"]
       }
    return tool_dict

def get_str_id(voca_map, target_str):
    if target_str in voca_map:
        return voca_map[target_str]
    for token, idx in voca_map.items():
        if token == target_str:
            return idx
    return None

def get_num_ids(voca_map):
    num_ids = []
    for token, idx in voca_map.items():
        if token.isdigit() or token == '.':
            num_ids.append(idx)
    return num_ids

def get_next_tokenid(llm, allowed_ids, input_ids):
    logits = llm.get_logits_from_input_ids(input_ids)
    filtered_logits = [-float('inf')] * len(logits)
    for idx in allowed_ids:
        if 0 <= idx < len(logits):
            filtered_logits[idx] = logits[idx]
    
    if max(filtered_logits) == -float('inf'):
        best_id = logits.index(max(logits))
    else:
        best_id = filtered_logits.index(max(filtered_logits))
    
    return best_id

def get_func_name(llm, system_prompt, current_json, tools_dict):
    """
    ç”Ÿæˆå‡½æ•°åå¹¶è¿”å›å‡½æ•°åå’Œæ›´æ–°åçš„ JSON å­—ç¬¦ä¸²
    
    å‚æ•°:
        llm: è¯­è¨€æ¨¡å‹å®ä¾‹
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        current_json: å½“å‰ JSON å­—ç¬¦ä¸²
        tools_dict: å·¥å…·å­—å…¸
    
    è¿”å›:
        (func_name, current_json): å‡½æ•°åå’Œæ›´æ–°åçš„ JSON å­—ç¬¦ä¸²
    """
    # === ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‡½æ•°å ===
    # ä¸ºæ¯ä¸ªå‡½æ•°åç”Ÿæˆ token åºåˆ—
    func_name_tokens = {}
    for fn_name in tools_dict.keys():
        # ç¼–ç å‡½æ•°åï¼Œå¾—åˆ° token åˆ—è¡¨
        tokens = llm._encode(fn_name)[0].tolist()
        func_name_tokens[fn_name] = tokens

    full_context = system_prompt + current_json
    input_ids = llm._encode(full_context)[0].tolist()
    
    generated_func_name = ""
    max_func_name_length = max(len(tokens) for tokens in func_name_tokens.values())

    for token_pos in range(max_func_name_length):
        # æ‰¾å‡ºåœ¨å½“å‰ä½ç½®ï¼Œå“ªäº›å‡½æ•°åè¿˜æ˜¯å€™é€‰
        allowed_token_ids = set()
        
        for fn_name, tokens in func_name_tokens.items():
            # æ£€æŸ¥å·²ç”Ÿæˆçš„éƒ¨åˆ†æ˜¯å¦åŒ¹é…è¿™ä¸ªå‡½æ•°åçš„å¼€å¤´
            if token_pos < len(tokens):
                # æ£€æŸ¥å‰é¢çš„ token æ˜¯å¦åŒ¹é…
                current_tokens = llm._encode(generated_func_name)[0].tolist() if generated_func_name else []
                expected_tokens = tokens[:token_pos]
                
                if current_tokens == expected_tokens or token_pos == 0:
                    # è¿™ä¸ªå‡½æ•°åè¿˜æ˜¯å€™é€‰ï¼Œå…è®¸å®ƒçš„ä¸‹ä¸€ä¸ª token
                    allowed_token_ids.add(tokens[token_pos])
        
        if not allowed_token_ids:
            break
        
        # ä»å…è®¸çš„ token ä¸­é€‰æ‹©æœ€ä¼˜çš„
        next_token_id = get_next_tokenid(llm, list(allowed_token_ids), input_ids)
        next_token_str = llm._decode([next_token_id])
        
        generated_func_name += next_token_str
        current_json += next_token_str  # ä¿®æ”¹å±€éƒ¨å˜é‡
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        full_context = system_prompt + current_json
        input_ids = llm._encode(full_context)[0].tolist()
        
        # æ£€æŸ¥æ˜¯å¦å®Œæ•´åŒ¹é…äº†æŸä¸ªå‡½æ•°å
        if generated_func_name in tools_dict:
            break
    
    func_name = generated_func_name
    print(f"Predicted function name: {func_name}\n")
    
    # ğŸ”‘ å…³é”®ï¼šè¿”å›å‡½æ•°åå’Œæ›´æ–°åçš„ current_json
    return func_name, current_json




def constrained_generation(llm, prompt, schema, voca_map):
    tools_dict = build_tool_dict(schema)

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼Œå‘Šè¯‰æ¨¡å‹è¦ç”Ÿæˆ JSON æ ¼å¼çš„å‡½æ•°è°ƒç”¨
    # å…³é”®ï¼šè®©æ¨¡å‹ä» prompt ä¸­æå–å‚æ•°å€¼
    system_prompt = (
        f"Extract function call from query.\n"
        f"Query: {prompt}\n"
        f"Available functions: {list(tools_dict.keys())}\n"
        f"Extract exact values from the query and format as JSON.\n"
        f"JSON: "
    )
    
    current_json = '{"fn_name": "'

    # === ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‡½æ•°å ===
    func_name, current_json = get_func_name(llm, system_prompt, current_json, tools_dict)

    # === ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå‚æ•° ===
    transition_str = '", "args": {'
    current_json += transition_str
    
    full_context = system_prompt + current_json
    input_ids = llm._encode(full_context)[0].tolist()

    if func_name in tools_dict:
        args_names = tools_dict[func_name]["args_names"]
        args_types = tools_dict[func_name]["args_types"]

        for i, arg_name in enumerate(args_names):
            current_json += f'"{arg_name}": '
            
            # æ›´æ–°å®Œæ•´ä¸Šä¸‹æ–‡
            full_context = system_prompt + current_json
            input_ids = llm._encode(full_context)[0].tolist()

            arg_type = args_types.get(arg_name, "str")
            
            # æ ¹æ®å‚æ•°ç±»å‹ç”Ÿæˆå€¼
            if arg_type in ["float", "int"]:
                # å¯¹äºæ•°å­—ç±»å‹ï¼Œéœ€è¦é€ä¸ª token ç”Ÿæˆå®Œæ•´çš„æ•°å­—
                generated_value = ""
                max_number_tokens = 5  # é™åˆ¶ä¸º 5 ä¸ª tokenï¼ˆé¿å…ç”Ÿæˆè¿‡é•¿æ•°å­—ï¼‰
                
                for token_idx in range(max_number_tokens):
                    allowed_val_ids = get_num_ids(voca_map)
                    
                    if not allowed_val_ids:
                        break
                    
                    val_id = get_next_tokenid(llm, allowed_val_ids, input_ids)
                    val_token = llm._decode([val_id])
                    
                    # åªä¿ç•™æ•°å­—å’Œå°æ•°ç‚¹å­—ç¬¦
                    clean_token = ''.join(c for c in val_token if c in '0123456789.')
                    
                    # å¦‚æœ token ä¸ºç©ºæˆ–åŒ…å«éæ•°å­—å­—ç¬¦ï¼Œåœæ­¢
                    if not clean_token:
                        break
                    
                    generated_value += clean_token
                    current_json += clean_token
                    
                    # æ›´æ–°ä¸Šä¸‹æ–‡ç»§ç»­ç”Ÿæˆ
                    full_context = system_prompt + current_json
                    input_ids = llm._encode(full_context)[0].tolist()
                    
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„ token
                    next_logits = llm.get_logits_from_input_ids(input_ids)
                    next_best_id = next_logits.index(max(next_logits))
                    next_token = llm._decode([next_best_id])
                    
                    # å¦‚æœä¸‹ä¸€ä¸ª token ä¸æ˜¯æ•°å­—ï¼Œåœæ­¢
                    if not any(c in next_token for c in '0123456789.'):
                        break
                
                # å¦‚æœæ²¡ç”Ÿæˆä»»ä½•æ•°å­—ï¼Œä½¿ç”¨é»˜è®¤å€¼
                if not generated_value:
                    generated_value = "0"
                    current_json += "0"
                    
            else:
                # å¯¹äºå­—ç¬¦ä¸²ç±»å‹ï¼Œç”Ÿæˆå¸¦å¼•å·çš„å­—ç¬¦ä¸²
                current_json += '"'
                full_context = system_prompt + current_json
                input_ids = llm._encode(full_context)[0].tolist()
                
                generated_value = ""
                max_string_tokens = 15  # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
                
                for token_idx in range(max_string_tokens):
                    # å…è®¸æ‰€æœ‰ token
                    allowed_val_ids = range(len(voca_map))
                    
                    val_id = get_next_tokenid(llm, allowed_val_ids, input_ids)
                    val_token = llm._decode([val_id])
                    
                    # å¦‚æœé‡åˆ°å¼•å·æˆ–é€—å·ï¼Œè¯´æ˜å‚æ•°å€¼ç»“æŸ
                    if '"' in val_token or ',' in val_token or '}' in val_token:
                        break
                    
                    generated_value += val_token
                    current_json += val_token
                    
                    # æ›´æ–°ä¸Šä¸‹æ–‡
                    full_context = system_prompt + current_json
                    input_ids = llm._encode(full_context)[0].tolist()
                
                current_json += '"'

            if i < len(args_names) - 1:
                current_json += ", "

        current_json += "}}"  # å…³é—­ args å’Œæ•´ä¸ª JSON

    else:
        # å¦‚æœå‡½æ•°åä¸åœ¨å­—å…¸ä¸­ï¼Œå…³é—­ JSON
        current_json += "}}"

    try:
        return json.loads(current_json)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        print(f"Generated JSON string: {current_json}")
        return {"fn_name": func_name, "args": {}}


def is_json_complete(text):
    if not text:
        return False
    
    open_braces = text.count('{')
    close_braces = text.count('}')
    if open_braces > 0 and open_braces == close_braces:
        return True
    return False

def main():
    llm = Small_LLM_Model()  # ä¿®å¤ï¼šç›´æ¥å®ä¾‹åŒ–ï¼Œä¸è¦è°ƒç”¨ __init__()

    with open("data/exercise_input/function_calling_tests.json", "r") as f:
        prompts = json.load(f)
    with open("data/exercise_input/functions_definition.json", "r") as f:
        schema = json.load(f)

    voca_path = llm.get_path_to_vocabulary_json()
    with open(voca_path, "r") as f:
        voca_map = json.load(f)

    outputs = []
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = "data/exercise_output"
    os.makedirs(output_dir, exist_ok=True)

    for prompt in prompts:
        prompt_str = prompt["prompt"]
        print(f"prompt: {prompt_str}")

        res_json = constrained_generation(llm, prompt_str, schema, voca_map)
        output_entry = {
            "prompt": prompt_str,
            "fn_name": res_json.get("fn_name", ""),
            "args": res_json.get("args", {})
        }
        outputs.append(output_entry)

    output_path = os.path.join(output_dir, "function_calling_results.json")
    with open(output_path, "w") as output_file:
        json.dump(outputs, output_file, indent=4)
    
    print(f"\nâœ… Results saved to: {output_path}")
    print(f"ğŸ“Š Processed {len(outputs)} prompts")


if __name__ == "__main__":
    main()